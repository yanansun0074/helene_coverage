{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect articles about hurricane Helene coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use wayback Machine to scrape everything from a news website\n",
    "\n",
    "2. Use UNC access's News Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. News Bank pdf about \"Helene\" coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Selenium to download news article pdfs from news bank\n",
    "\n",
    "keyword: helene\n",
    "\n",
    "date: Sep 2024 (when the first 3 articles about Helene were published) - Oct 2024?\n",
    "\n",
    "Location: USA - North Carolina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Module\n",
    "import selenium.webdriver as webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By \n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Chrome\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open URL\n",
    "url = \"https://infoweb.newsbank.com/apps/news/results?hide_duplicates=2&offset=0&maxresults=60&sort=YMD_date%3AD&p=AMNEWS&t=country%3AUSA%21USA/state%3ANC%21USA%2B-%2BNorth%2BCarolina&f=advanced&val-base-0=helene&fld-base-0=alltext&bln-base-1=and&val-base-1=Sep%202024%20-%20Oct%202024&fld-base-1=YMD_date\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "# Download pdf\n",
    "try:\n",
    "    # For I pages\n",
    "    for i in range(2):\n",
    "        # Click \"select all\" box\n",
    "        # await driver.find_element(By.CLASS_NAME, 'search-hits__select-all form-checkbox').click()\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"search-hits__select-all.form-checkbox\"))).click()\n",
    "        # Click next\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"pager__item.pager__item--next\"))).click()\n",
    "        time.sleep(1)\n",
    "except Exception as e:\n",
    "    print(f\"Cannot select page {i}\")\n",
    "    print(e)\n",
    "else:\n",
    "    # Click download button\n",
    "    driver.find_element(By.CLASS_NAME, 'actions-bar__button.actions-bar__button--download').click()\n",
    "    time.sleep(1)\n",
    "    # While there is \"Next\" button\n",
    "    while True:\n",
    "        # Click \"Download\" button in pop-up\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"u-button multidoc-download__button\"))).click()\n",
    "        # wait loader to appear and then disappear\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_selected((By.CLASS_NAME, 'multidoc__button-spinner')))\n",
    "        WebDriverWait(driver, 10).until(EC.invisibility_of_element((By.CLASS_NAME, 'multidoc__button-spinner')))\n",
    "        # Click \"Next\"\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"multidocs_pager_nav next\"))).click()\n",
    "        except:\n",
    "            print(\"No Next found\")\n",
    "            break\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is security check of robot. Can't pass ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ended up with manually downloading 5000+ articles by groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import selenium.webdriver as webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By \n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract text from pdf with <a href = 'https://github.com/jsvine/pdfplumber'>pdfplumber</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   FUNC extract_text_from_pdf()\n",
    "#   Input: directory (str) of pdf\n",
    "#   Output: header (list) - article header information\n",
    "#         article (list) - article text\n",
    "def extract_text_from_pdf(dir, headers, articles):\n",
    "    # header, article = [],[]\n",
    "    with pdfplumber.open(dir) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # extract texts on each page and split texts by \"OpenURL Link\"\n",
    "            texts = page.extract_text().split(\"\\nOpenURL Link\\n\")\n",
    "            # there is no \"OpenURL Link\" on that page\n",
    "            if len(texts) == 1:\n",
    "                articles[-1] += texts[0]\n",
    "            else:\n",
    "                headers.append(texts[0])\n",
    "                articles.append(texts[1])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   FUNC: decompose_header\n",
    "#   Input: header (str)\n",
    "#   Output: append titles, dates, newspapers, authors, and word_counts\n",
    "def decompose_header(header):\n",
    "\n",
    "\n",
    "    date_match = re.search(r\"\\b(September|October) \\d{1,2}, \\d{4}\\b\", header)\n",
    "    \n",
    "    date = date_match.group(0) if date_match else \"\"\n",
    "    dates.append(date)\n",
    "    loc = header.find(date)\n",
    "    titles.append(header[:loc].replace(\"\\n\", \"\"))\n",
    "\n",
    "    texts = header[loc:].split('\\n')\n",
    "    newspapers.append(texts[0][len(date)+1:].replace('| ', \"\"))\n",
    "\n",
    "    # Line three\n",
    "    match = re.search(r\"Author: (.*?)Section: \", texts[1])\n",
    "    author = match.group(1) if match else \"\"\n",
    "    authors.append(author)\n",
    "    match_word = re.search(r\"(\\d+)\\s*Words\", texts[1])\n",
    "    word_count = match_word.group(1) if match_word else \"\"\n",
    "    word_counts.append(word_count)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files under news_bank_pdf\n",
    "directory = 'news_bank_pdf'\n",
    "pdf_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\".pdf\")]\n",
    "\n",
    "headers, articles = [],[]\n",
    "# Extract articles and headers from pdfs\n",
    "for path in pdf_paths:\n",
    "    extract_text_from_pdf(path, headers, articles)\n",
    "\n",
    "# Save articles and headers to dataframe\n",
    "temp = pd.DataFrame({\"header\":headers,\n",
    "                    \"article\": articles})\n",
    "\n",
    "# temp.to_csv('articles_20241112.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose headers\n",
    "titles, dates, newspapers, authors, word_counts = [],[],[],[],[]\n",
    "temp['header'].apply(lambda x: decompose_header(x))\n",
    "\n",
    "temp['title'] = titles\n",
    "temp['date'] = dates\n",
    "temp['newspaper'] = newspapers\n",
    "temp['author'] = authors\n",
    "temp['word_count'] = word_counts\n",
    "temp.head()\n",
    "\n",
    "# temp.to_csv(\"articles_20241112_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_tru_link(url):\n",
    "    # Open Chrome\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Open URL\n",
    "    # url = \"https://infoweb.newsbank.com/apps/news/results?hide_duplicates=2&offset=0&maxresults=60&sort=YMD_date%3AD&p=AMNEWS&t=country%3AUSA%21USA/state%3ANC%21USA%2B-%2BNorth%2BCarolina&f=advanced&val-base-0=helene&fld-base-0=alltext&bln-base-1=and&val-base-1=Sep%202024%20-%20Oct%202024&fld-base-1=YMD_date\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"search-hits__select-all.form-checkbox\"))).click()\n",
    "    driver.quit()\n",
    "    return\n",
    "\n",
    "extract_text_tru_link('https://infoweb.newsbank.com/apps/news/user/login?destination=results%3Fhide_duplicates%3D2%26offset%3D0%26maxresults%3D60%26sort%3DYMD_date%253AD%26p%3DAMNEWS%26t%3Dcountry%253AUSA%2521USA/state%253ANC%2521USA%252B-%252BNorth%252BCarolina%26f%3Dadvanced%26val-base-0%3Dhelene%26fld-base-0%3Dalltext%26bln-base-1%3Dand%26val-base-1%3DSep%25202024%2520-%2520Oct%25202024%26fld-base-1%3DYMD_date')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from newspaper layout with pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(\"News_Article__Fayetteville_Observer_The_NC___October_8_2024__p1.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text(x_tolerance=2, y_tolerance=2)  # Adjust for layout\n",
    "        print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from newspaper layout with PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Open the PDF file\n",
    "pdf = fitz.open(\"News_Article__Fayetteville_Observer_The_NC___October_8_2024__p1.pdf\")\n",
    "\n",
    "for page_num in range(pdf.page_count):\n",
    "    page = pdf[page_num]\n",
    "    blocks = page.get_text(\"blocks\")  # Extract text blocks with coordinates\n",
    "\n",
    "    # Sort blocks by y-coordinate for columnar layout\n",
    "    blocks.sort(key=lambda block: (block[1], block[0]))  # y, x sorting\n",
    "    for block in blocks:\n",
    "        print(block[4])  # Text in each block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from newspaper layout with Tesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install <a href='https://github.com/tesseract-ocr/tesseract'>Tesseract</a> through homebrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PDF pages to images\n",
    "pages = convert_from_path(\"News_Article__Fayetteville_Observer_The_NC___October_8_2024__p1.pdf\")\n",
    "\n",
    "for page in pages:\n",
    "    # Crop or split image into columns if necessary\n",
    "    text = pytesseract.image_to_string(page, config='--psm 6')  # PSM 6 for uniform block detection\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from newspaper layout with Layout parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layoutparser as lp\n",
    "from layoutparser.models.detectron2 import catalog\n",
    "import copy\n",
    "import os\n",
    "import requests as requests\n",
    "import layoutparser as lp\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def load_model(\n",
    "        config_path: str = 'lp://<dataset_name>/<model_name>/config',\n",
    "):\n",
    "\n",
    "    config_path_split = config_path.split('/')\n",
    "    dataset_name = config_path_split[-3]\n",
    "    model_name = config_path_split[-2]\n",
    "\n",
    "    # get the URLs from the MODEL_CATALOG and the CONFIG_CATALOG \n",
    "    # (global variables .../layoutparser/models/detectron2/catalog.py)\n",
    "    model_url = catalog.MODEL_CATALOG[dataset_name][model_name]\n",
    "    config_url = catalog.CONFIG_CATALOG[dataset_name][model_name]\n",
    "\n",
    "    # override folder destination:\n",
    "    if 'model' not in os.listdir():\n",
    "        os.mkdir('model')\n",
    "\n",
    "    config_file_path, model_file_path = None, None\n",
    "\n",
    "    for url in [model_url, config_url]:\n",
    "        filename = url.split('/')[-1].split('?')[0]\n",
    "        save_to_path = f\"model/\" + filename\n",
    "        if 'config' in filename:\n",
    "            config_file_path = copy.deepcopy(save_to_path)\n",
    "        if 'model_final' in filename:\n",
    "            model_file_path = copy.deepcopy(save_to_path)\n",
    "\n",
    "        # skip if file exist in path\n",
    "        if filename in os.listdir(\"model\"):\n",
    "            continue\n",
    "        # Download file from URL\n",
    "        r = requests.get(url, stream=True, headers={'user-agent': 'Wget/1.16 (linux-gnu)'})\n",
    "\n",
    "        with open(save_to_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=4096):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    # load the label map\n",
    "    label_map = catalog.LABEL_MAP_CATALOG[dataset_name]\n",
    "\n",
    "    return lp.models.Detectron2LayoutModel(\n",
    "        config_path=config_file_path,\n",
    "        model_path=model_file_path,\n",
    "        label_map=label_map\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image of the PDF page (or convert PDF to image as above)\n",
    "# image = Image.open(\"page_image.png\")\n",
    "pages = convert_from_path(\"News_Article__Fayetteville_Observer_The_NC___October_8_2024__p1.pdf\")\n",
    "\n",
    "# Error occurs: having trouble finding the model\n",
    "# layout = lp.models.Detectron2LayoutModel(\"lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config\")\n",
    "# layout = lp.models.Detectron2LayoutModel(\"lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config\")\n",
    "# layout = lp.Detectron2LayoutModel(\n",
    "# \"./config.yaml\",\n",
    "# \"./model_final.pth\",\n",
    "# extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8],\n",
    "# label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3: \"Table\", 4: \"Figure\"},\n",
    "# )\n",
    "\n",
    "# This works\n",
    "# layout = load_model('lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config')\n",
    "\n",
    "\n",
    "detected_layout = layout.detect(pages[0])\n",
    "\n",
    "for block in detected_layout:\n",
    "    segment = block.extract(pages)\n",
    "    text = pytesseract.image_to_string(segment)  # OCR on each segment\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
