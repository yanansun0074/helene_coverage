{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect articles about hurricane Helene coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use wayback Machine to scrape everything from a news website\n",
    "\n",
    "2. Use UNC access's News Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. News Bank pdf about \"Helene\" coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Selenium to download news article pdfs from news bank\n",
    "\n",
    "keyword: helene\n",
    "\n",
    "date: Sep 2024 (when the first 3 articles about Helene were published) - Oct 2024?\n",
    "\n",
    "Location: USA - North Carolina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Module\n",
    "import selenium.webdriver as webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By \n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Chrome\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open URL\n",
    "url = \"https://infoweb.newsbank.com/apps/news/results?hide_duplicates=2&offset=0&maxresults=60&sort=YMD_date%3AD&p=AMNEWS&t=country%3AUSA%21USA/state%3ANC%21USA%2B-%2BNorth%2BCarolina&f=advanced&val-base-0=helene&fld-base-0=alltext&bln-base-1=and&val-base-1=Sep%202024%20-%20Oct%202024&fld-base-1=YMD_date\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "# Download pdf\n",
    "try:\n",
    "    # For I pages\n",
    "    for i in range(2):\n",
    "        # Click \"select all\" box\n",
    "        # await driver.find_element(By.CLASS_NAME, 'search-hits__select-all form-checkbox').click()\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"search-hits__select-all.form-checkbox\"))).click()\n",
    "        # Click next\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"pager__item.pager__item--next\"))).click()\n",
    "        time.sleep(1)\n",
    "except Exception as e:\n",
    "    print(f\"Cannot select page {i}\")\n",
    "    print(e)\n",
    "else:\n",
    "    # Click download button\n",
    "    driver.find_element(By.CLASS_NAME, 'actions-bar__button.actions-bar__button--download').click()\n",
    "    time.sleep(1)\n",
    "    # While there is \"Next\" button\n",
    "    while True:\n",
    "        # Click \"Download\" button in pop-up\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"u-button multidoc-download__button\"))).click()\n",
    "        # wait loader to appear and then disappear\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_selected((By.CLASS_NAME, 'multidoc__button-spinner')))\n",
    "        WebDriverWait(driver, 10).until(EC.invisibility_of_element((By.CLASS_NAME, 'multidoc__button-spinner')))\n",
    "        # Click \"Next\"\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"multidocs_pager_nav next\"))).click()\n",
    "        except:\n",
    "            print(\"No Next found\")\n",
    "            break\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is security check of robot. Can't pass ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape day by day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import selenium.webdriver as webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By \n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract text from pdf with <a href = 'https://github.com/jsvine/pdfplumber'>pdfplumber</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   FUNC extract_text_from_pdf()\n",
    "#   Input: directory (str) of pdf\n",
    "#   Output: header (list) - article header information\n",
    "#         article (list) - article text\n",
    "def extract_text_from_pdf(dir, headers, articles):\n",
    "    # header, article = [],[]\n",
    "    with pdfplumber.open(dir) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # extract texts on each page and split texts by \"OpenURL Link\"\n",
    "            texts = page.extract_text().split(\"\\nOpenURL Link\\n\")\n",
    "            # there is no \"OpenURL Link\" on that page\n",
    "            if len(texts) == 1:\n",
    "                articles[-1] += texts[0]\n",
    "            else:\n",
    "                headers.append(texts[0])\n",
    "                articles.append(texts[1])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNC: decompose_header\n",
    "# Input: header (str)\n",
    "# Output: append titles, dates, newspapers, authors, and word_counts\n",
    "def decompose_header(header):\n",
    "\n",
    "\n",
    "    date_match = re.search(r\"\\b(September|October) \\d{1,2}, \\d{4}\\b\", header)\n",
    "    \n",
    "    date = date_match.group(0) if date_match else \"\"\n",
    "    dates.append(date)\n",
    "    loc = header.find(date)\n",
    "    titles.append(header[:loc])\n",
    "\n",
    "    texts = header[loc:].split('\\n')\n",
    "    newspapers.append(texts[0][len(date)+1:])\n",
    "\n",
    "    # Line three\n",
    "    match = re.search(r\"Author: (.*?)Section: \", texts[1])\n",
    "    author = match.group(1) if match else \"\"\n",
    "    authors.append(author)\n",
    "    match_word = re.search(r\"(\\d+)\\s*Words\", texts[1])\n",
    "    word_count = match_word.group(1) if match_word else \"\"\n",
    "    word_counts.append(word_count)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files under news_bank_pdf\n",
    "directory = 'news_bank_pdf'\n",
    "pdf_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\".pdf\")]\n",
    "\n",
    "headers, articles = [],[]\n",
    "# Extract articles and headers from pdfs\n",
    "for path in pdf_paths:\n",
    "    extract_text_from_pdf(pdf_paths[0], headers, articles)\n",
    "\n",
    "# Save articles and headers to dataframe\n",
    "temp = pd.DataFrame({\"header\":headers,\n",
    "                    \"article\": articles})\n",
    "\n",
    "# temp.to_csv('articles_20241107.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose headers\n",
    "titles, dates, newspapers, authors, word_counts = [],[],[],[],[]\n",
    "temp['header'].apply(lambda x: decompose_header(x))\n",
    "\n",
    "temp['title'] = titles\n",
    "temp['date'] = dates\n",
    "temp['newspaper'] = newspapers\n",
    "temp['author'] = authors\n",
    "temp['word_count'] = word_counts\n",
    "temp.head()\n",
    "\n",
    "# temp.to_csv(\"articles_20241108.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_tru_link(url):\n",
    "    # Open Chrome\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Open URL\n",
    "    # url = \"https://infoweb.newsbank.com/apps/news/results?hide_duplicates=2&offset=0&maxresults=60&sort=YMD_date%3AD&p=AMNEWS&t=country%3AUSA%21USA/state%3ANC%21USA%2B-%2BNorth%2BCarolina&f=advanced&val-base-0=helene&fld-base-0=alltext&bln-base-1=and&val-base-1=Sep%202024%20-%20Oct%202024&fld-base-1=YMD_date\"\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"search-hits__select-all.form-checkbox\"))).click()\n",
    "    driver.quit()\n",
    "    return\n",
    "\n",
    "extract_text_tru_link('https://infoweb.newsbank.com/apps/news/user/login?destination=results%3Fhide_duplicates%3D2%26offset%3D0%26maxresults%3D60%26sort%3DYMD_date%253AD%26p%3DAMNEWS%26t%3Dcountry%253AUSA%2521USA/state%253ANC%2521USA%252B-%252BNorth%252BCarolina%26f%3Dadvanced%26val-base-0%3Dhelene%26fld-base-0%3Dalltext%26bln-base-1%3Dand%26val-base-1%3DSep%25202024%2520-%2520Oct%25202024%26fld-base-1%3DYMD_date')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
